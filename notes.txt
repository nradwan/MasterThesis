First paper:
--check out: Yamaguchi and Maruyama [5] propose a method to
extract character regions in natural scene images by
hierarchical classifiers. The hierarchy consists of two
types of classifiers: a histogram-based classifier and
SVM.
--check out: [7] [8] for the current paper to do the image binarization and enhancement (described in steps below)
--Steps::
+first preprocessing by using a low-pass wiener filter (for color images use the luminance component)
+use Sauvola's approach for adaptive thresholding with k=0.2 to get binary image S where 1 represents estimated foreground regions
+background surface estimation B: pixels which got value 0 in S, get a value in B equivalent to their value in O (original image), for the remaining pixels, the value is computed by neighbouring pixel interpolation.
+final thresholding: combine the background with the original image. Text areas have distance between O(x,y) and B(x,y) > d (threshold). d changes according to gray-scale value of B in order to preserve text information in dark background areas. (smaller d in darker regions)
+image upsampling: using bicubic interpolation. pixel in destination is estimated by average of 16 closest pixels to the corresponding pixel in source.
+image post-processing: use shrink and swell filtering
--Do previous steps for both original and negative gray scale image to ensure getting text information
--CCA (not fully understood yet)

Second paper:
--more similar to what is described in the paper by posner and newmann
--need more understanding still

Third paper:
--check out:  Niblack dynamic thresholding [6] as modified by Gatos et. al[1]==> this is in fact the first paper!!
--seems similar to first paper approach (maybe ask about the component segmentation or try reading more about it)

##Maybe use 1st paper approach combined with the filters described in the 3rd paper
===> basically just use the 3rd paper with the 1st paper as written!


NEW PLAN:
Follow the text-spotting paper!
Steps for the gradient paper (second paper above):
--Compute the gradient vector and the gradient magnitude
--Compute the Census Transform (check description in paper as how to)
--Define 5 partitions based on gradient based features. Compute the regions for each of the 4 pictures (3 gradient and 1 CT)
--Use cascade gentle adaboost classifiers with descision stump as the adaboost weaklearner
--Ask about last section before results


================================================================
Status Update 21/08:
--integrated libccv and tesseract together
--tesseract has better output with libccv
--sometimes some letters are cropped out of the text
--the confidence level returned by tesseract is not very accurate
--other text spotting method in opencv is still not available in stable opencv release version
--thought about training tesseract with some outdoor scene images along with the german language data to provide as input (feseability of this approach?)
--thought about using dictionary checks with probablistic background to correct the tesseract output string
--is it useful to not load the system-generated dictionary from tesseract in the case of recognizing names of streets/shops to increase the recognition value or not (needs inspection)
--using the google api to search for the text returned from the previous action ===> a) api uses javascript DONE!
																					 b) could use place searches DONE!
																					 c) need to fix api key DONE!
--maybe merge the output of the textspotting (merge the bounding boxes) to get better outcome?
--thought about use probabilistic error correction for words resulting from Tesseract (see Posner paper) ==>needs training data, not understood
--thought about contacting authors of paper to get Street View Dataset they used in their validation (DONE but data in US not zurich)
--try to match the logo you get from the nearby search results with anything in the image to see if we are at this location or not using sift features DONE!
--download new data to start the search on (NEEDS TO BE DONE)
--we cannot use the photo result currently used as it returns an image from the map not of the true place. Try using the search api for photos instead DONE!
--also consider using the logopedia output for the place (NEEDS TO BE DONE) ==>would have same problem as below!
--could happen that ocr identifies correctly but the place is simply not listed
--consider using google search to correct the api output DONE! PS Must link words from ocr by "+"
--prepare entire example to show next time DONE
--need to handle any exceptions thrown DONE!
--how do we handle the output from multiple boxes? concatenate into one word or split into several? Maybe depending on distance between bounding boxes, maybe handeled on it's own from the search api DONE
--also need to handle cases where the search returns no results at all, in case of very noisy ocr results DONE
--tokenize the search into maximal 3 terms together (in case results from ocr is more than 3 boxes), perform text correction on them first! DONE
--PROBLEM: correction step may not correct all text and then we would get trouble in nearby place search phase
--Possible solution: do nearby search on permutations of maximum 3 words.
--PROBLEM: photos by google are not always useful in identifying the place as sometimes it's just the logo of the place which is not visible in the image. Resulting in matching with wrong matches even when the correct place was in the result.
--PROBLEM: need to find better way instead of calling all computations as it eventually gets expensive.
--PROBLEM: sometimes the place doesn't have an associated photo.
--Maybe use Franken+ to train tesseract but not sure if it's compatible on linux or not.
--Do the other way around, search the area around the gps location and get the matching places, then try to see which matches best with the ocr result we get DONE
--Integrate the hunspell dictionary to get word corrections/suggestions--maybe can help in the matching phase?/useful also as we only get 100 requests allowed per day for the custom search api to do the spell correction (NEEDS TO BE DONE *& investigated*)
--Take a street with lots of shops, then run the query sequentially on data from walking in the street, and run/implement kalman filter to localize and smooth the results. 
	--STEPS::
			++download data from lots of shops in one street
			++implement method to take the data sequentially from them DONE
			++implement kalman filter DONE
			++implement visualization method
	--Notes for kalman filter:
			++create file that has longitude/latitude of each timestep followed by image name that corresponds to it (in order of visiting)
			++the observations from the nearby place search
			++we need to specify keywords that would be working for the nearby search
			++use less noise for odometry than for observations as they're not really accurate depending on the text spotting and the ocr step DONE
			++implement motion model DONE
			++implement correction step DONE
			++convert gps to xytheta DONE
			++Observation is the location of the shop, and the odometry is the gps coordinates when we took the picture minus gps coordinates of where we took the previous picture
			++increase the size of the matrix as we go and initialize them!! (DONE, but only by 1 as we have just 1 observation per step)
			++implement main method DONE
			++test motion model (simple testing) DONE
			++test correction step (simple testing) DONE
			--IN THE MAIN METHOD: DONE
					++loop over the data file
					++data file should be filled with the gps coordinates (need one datapoint extra for initialization)
					++convert them to odometry
					++call the motion model
					++load the picture then call the nearby search with the keyword
					++adjust the nearby search to return gps location
					++get the location from the returned result
					++convert to odometry
					++call the correction step
			++TESTING AND CREATING THE FILES DONE
	++Visualization:
		--connect qt to the project DONE
		--create an interface for viewing the image from the google static maps DONE
		--create a function that adds a new point connecting it to a path from the code DONE
		--always append to the url you have currently the newly added value DONE
		--call the function from the code everytime a new point has been calculated DONE
		--static maps from google maps, to highlight the path DONE
		--add print to file!! DONE
	++Notes:
			--think about multiple keywords being integrated
			--maybe having multiple observations (right now we just get only the best search place result)
			--try ranking maybe not just based on keyword match score but also incorporate distance from gps signal given
	++Add serialization to cache the search data DONE
	++Change the view to openstreetmap roadview DONE
	++Imporve the text matching part as it finds the text but gives a bigger score to other matches. (implement edit distance algorithm to do the matching) DONE
	++Find a way to use better keywords as they have a huge influence on the output of the algorithm.
	++Find a way to crop detected non-words as they mess up the matching with the edit distance algorithm
	++Maybe find a way to only consider keywords that are closest to our current position (cut off after certain distance) DONE EXCLUDING ALL LOCATIONS AT A DISTANCE GREATER THAN 1 KM FROM GPS
	++Change the query from searching for a specific keyword to searching for everything in the nearbyloc DONE
	++Maybe set maximum edit distance after which declare failure to match DONE
	++In such case do query expansion
	++Read about query expansion
	
	ROUGH IDEA: DONE
	Maybe look for different technique to use that ranks the results better than nearby place search
	Maybe combine both approaches psuedo relevance feedback and query expansion
	--Use nearby search with keyword * with radius threshold
	--Filter the results either taking the type of the first n as positive results and bottom 20-n as negative results
	--Research with this type
	--Do the matching
	OR
	--Use nearby search with detected word with radius threshold
	--If the edit distance match is greater than x,
	--Find synonyms of the words and re-do the search via thesaurus or wordnet (use only all synonyms in synset and hypernyms, however is not reported to be consistently useful)
	--Do the matching
	
	TODO:
	++If no synonyms found then match with the result of the ocr! DONE
	++Store the confidence and then search for the word with the most confidence level! DONE
	++The result of the nearbysearch is arranged by distance (Closest first), if an item is occuring more than once in the result, and with high ranking ===> most probably this is the place we are searching for. (What happens if the other places have lower match socre?) Maybe have some sort of balance between matching score and number of occurences of an item. 
			
	NEW GOAL IDEAS:
	++can use text search to know how many places are available with this kewyord in a certain area. (Use next page token to count all search results).
	++can also be used to count the prior of this place in the are by adding the gps location parameter.
	++need to consider buying a license for training the regressor.
	++but if we use a regressor what would be the y value (should it be manually set? the ground truth)
	++Maybe use q-learning instead of regression where the reward would be the match percentage (the higher,the higher reward you have)
	++Split the data into different trials (number of images each), and the goal would be by the last image to have the maximum reward
