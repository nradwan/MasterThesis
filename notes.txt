First paper:
--check out: Yamaguchi and Maruyama [5] propose a method to
extract character regions in natural scene images by
hierarchical classifiers. The hierarchy consists of two
types of classifiers: a histogram-based classifier and
SVM.
--check out: [7] [8] for the current paper to do the image binarization and enhancement (described in steps below)
--Steps::
+first preprocessing by using a low-pass wiener filter (for color images use the luminance component)
+use Sauvola's approach for adaptive thresholding with k=0.2 to get binary image S where 1 represents estimated foreground regions
+background surface estimation B: pixels which got value 0 in S, get a value in B equivalent to their value in O (original image), for the remaining pixels, the value is computed by neighbouring pixel interpolation.
+final thresholding: combine the background with the original image. Text areas have distance between O(x,y) and B(x,y) > d (threshold). d changes according to gray-scale value of B in order to preserve text information in dark background areas. (smaller d in darker regions)
+image upsampling: using bicubic interpolation. pixel in destination is estimated by average of 16 closest pixels to the corresponding pixel in source.
+image post-processing: use shrink and swell filtering
--Do previous steps for both original and negative gray scale image to ensure getting text information
--CCA (not fully understood yet)

Second paper:
--more similar to what is described in the paper by posner and newmann
--need more understanding still

Third paper:
--check out:  Niblack dynamic thresholding [6] as modified by Gatos et. al[1]==> this is in fact the first paper!!
--seems similar to first paper approach (maybe ask about the component segmentation or try reading more about it)

##Maybe use 1st paper approach combined with the filters described in the 3rd paper
===> basically just use the 3rd paper with the 1st paper as written!


NEW PLAN:
Follow the text-spotting paper!
Steps for the gradient paper (second paper above):
--Compute the gradient vector and the gradient magnitude
--Compute the Census Transform (check description in paper as how to)
--Define 5 partitions based on gradient based features. Compute the regions for each of the 4 pictures (3 gradient and 1 CT)
--Use cascade gentle adaboost classifiers with descision stump as the adaboost weaklearner
--Ask about last section before results


================================================================
Status Update 21/08:
--integrated libccv and tesseract together
--tesseract has better output with libccv
--sometimes some letters are cropped out of the text
--the confidence level returned by tesseract is not very accurate
--other text spotting method in opencv is still not available in stable opencv release version
--thought about training tesseract with some outdoor scene images along with the german language data to provide as input (feseability of this approach?)
--thought about using dictionary checks with probablistic background to correct the tesseract output string
--is it useful to not load the system-generated dictionary from tesseract in the case of recognizing names of streets/shops to increase the recognition value or not (needs inspection)
--using the google api to search for the text returned from the previous action ===> a) api uses javascript DONE!
																					 b) could use place searches DONE!
																					 c) need to fix api key DONE!
--maybe merge the output of the textspotting (merge the bounding boxes) to get better outcome?
--thought about use probabilistic error correction for words resulting from Tesseract (see Posner paper) ==>needs training data, not understood
--thought about contacting authors of paper to get Street View Dataset they used in their validation (DONE but data in US not zurich)
--try to match the logo you get from the nearby search results with anything in the image to see if we are at this location or not using sift features DONE!
--download new data to start the search on (NEEDS TO BE DONE)
--we cannot use the photo result currently used as it returns an image from the map not of the true place. Try using the search api for photos instead DONE!
--also consider using the logopedia output for the place (NEEDS TO BE DONE) ==>would have same problem as below!
--could happen that ocr identifies correctly but the place is simply not listed
--consider using google search to correct the api output DONE! PS Must link words from ocr by "+"
--prepare entire example to show next time DONE
--need to handle any exceptions thrown DONE!
--how do we handle the output from multiple boxes? concatenate into one word or split into several? Maybe depending on distance between bounding boxes, maybe handeled on it's own from the search api DONE
--also need to handle cases where the search returns no results at all, in case of very noisy ocr results DONE
--tokenize the search into maximal 3 terms together (in case results from ocr is more than 3 boxes), perform text correction on them first! DONE
--PROBLEM: correction step may not correct all text and then we would get trouble in nearby place search phase
--Possible solution: do nearby search on permutations of maximum 3 words.
--PROBLEM: photos by google are not always useful in identifying the place as sometimes it's just the logo of the place which is not visible in the image. Resulting in matching with wrong matches even when the correct place was in the result.
--PROBLEM: need to find better way instead of calling all computations as it eventually gets expensive.
--PROBLEM: sometimes the place doesn't have an associated photo.
--Maybe use Franken+ to train tesseract but not sure if it's compatible on linux or not.
--Do the other way around, search the area around the gps location and get the matching places, then try to see which matches best with the ocr result we get (NEEDS TO BE DONE)
--Integrate the hunspell dictionary to get word corrections/suggestions--maybe can help in the matching phase?/useful also as we only get 100 requests allowed per day for the custom search api to do the spell correction (NEEDS TO BE DONE *& investigated*)

